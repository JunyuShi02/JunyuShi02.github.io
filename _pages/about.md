---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a second-year Ph.D. student in the Robotics and Autonomous Systems Thrust at The Hong Kong University of Science and Technology (Guangzhou), supervised by Prof. [Qiang Nie](https://scholar.google.com/citations?user=q-Qqa20AAAAJ&hl=en). Prior to this, I received my Bachelor's degree from the AI Special Class at Shenzhen University, under the supervision of Prof. [Wenming Cao](https://scholar.google.com/citations?user=uPxjSDIAAAAJ&hl=zh-CN).

My research currently centers on tasks related to human motion, specifically motion generation, prediction and understanding. In the next phase of my work, I aim to leverage large-scale human motion data to learn robotic motion priors for general-purpose control. Please feel free to email me at jshi890@connect.hkust-gz.edu.cn. Welcome to discuss and cooperate.


# üî• News
- *2025.11*: &nbsp;üéâüéâ One paper has been accepted at TriFusion Workshop in SIGGRAPH AISA 2025. 
- *2025.09*: &nbsp;üéâüéâ One paper has been accepted at AAAI 2026. Cong. to Yong Sun!
- *2025.06*: &nbsp;üéâüéâ One paper has been accepted at ICCV 2025. 
- *2025.05*: &nbsp;üéâüéâ One paper has been accepted early at MICCAI 2025. Cong. to Yong Sun!

# üìù Publications 

<div class='paper-box'>
<div class='paper-box-text' markdown="1">

[MoReFun: Past-Movement Guided Motion Representation Learning for Future Motion Prediction and Understanding](https://arxiv.org/abs/2408.02091), **Preprint**

**Junyu Shi**, Haoting Wu, Zhiyuan Zhang, Lijiang Liu, Yong Sun, Qiang Nie

</div>
</div>



<div class='paper-box'>
<div class='paper-box-text' markdown="1">

[MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722), **Preprint**

**Junyu Shi**, Yong Sun, Zhiyuan Zhang, Lijiang Liu, Zhengjie Zhang, Yuxin He, Qiang Nie

</div>
</div>


<div class='paper-box'>
<div class='paper-box-text' markdown="1">

[EmbryoDiff: A Conditional Diffusion Framework with Multi-Focal Feature Fusion for Fine-Grained Embryo Developmental Stage Recognition](https://arxiv.org/abs/2511.11027), **AAAI 2026**

Yong Sun, Zhengjie Zhang, **Junyu Shi**, Zhiyuan Zhang, Lijiang Liu, Qiang Nie

</div>
</div>


<div class='paper-box'>
<div class='paper-box-text' markdown="1">

[GenM^3: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation](https://openaccess.thecvf.com/content/ICCV2025/papers/Shi_GenM3_Generative_Pretrained_Multi-path_Motion_Model_for_Text_Conditional_Human_ICCV_2025_paper.pdf), **ICCV 2025**

**Junyu Shi**, Lijiang Liu, Yong Sun, Zhiyuan Zhang, Jinni Zhou, Qiang Nie

</div>
</div>



<div class='paper-box'>
<div class='paper-box-text' markdown="1">

[Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal Pattern Mining](https://arxiv.org/abs/2506.04950), **MICCAI 2025**

Yong Sun, Yipeng Wang, **Junyu Shi**, Zhiyuan Zhang, Yanmei Xiao, Lei Zhu, Manxi Jiang, Qiang Nie

</div>
</div>



<div class='paper-box'>
<div class='paper-box-text' markdown="1">

[RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics](https://arxiv.org/abs/2504.02069), **Preprint**

Zhiyuan Zhang, Yuxin He, Yong Sun, **Junyu Shi**, Lijiang Liu, Qiang Nie

</div>
</div>



<div class='paper-box'>
<div class='paper-box-text' markdown="1">

[Multi-semantics aggregation network based on the dynamic-attention mechanism for 3d human motion prediction](https://ieeexplore.ieee.org/document/10306327), **IEEE Transactions on Multimedia 2024**

**Junyu Shi**, Jianqi Zhong, Wenming Cao

</div>
</div>

